{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97f467f3-d793-4e95-ab7a-922126a88645",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-69-382cc5bebaac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-69-382cc5bebaac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install jupyter_contrib_nbextensions\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c91a257-1853-4b65-ba9d-7ddabbe45493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embedding\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c4aa9dd8-0328-4f75-85c6-2656a61a93c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/anirudh/Documents/CSCE 585/ML_chatbot_csce585')\n",
    "data_file = open('data/intentData/data_small.json').read()\n",
    "smallData = json.loads(data_file)\n",
    "oosValDocument =np.array( smallData['oos_val'])\n",
    "\n",
    "data_file = open('data/intent1.json').read()\n",
    "intents = json.loads(data_file)\n",
    "words = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['text']:\n",
    "        words.append(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c551f88-a127-466a-a5e7-92d975ded044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [adam, adios, aware, bay, bella, bored, bye, camera, cheering, clever, communicating, comprendo, conscious, doing, door, friends, fuck, genious, girl, good, goodbye, gossip, got, great, hear, hello, help, helpful, hi, hola, hope, hya, identify, intelligent, joke, jokes, know, later, laugh, make, mean, meant, need, ok, open, pod, prove, quiet, real, saying, self, shhh, shit, shut, speaking, stop, surely, talking, tell, thank, thanks, think, time, twat, understand, user, want, wasn]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 68 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': 'english',\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_1=\"This is a good job.I will not miss it for anything\"\n",
    "sentence_2=\"This is not good at all\"\n",
    "sentence = [sentence_1,sentence_2] \n",
    " \n",
    " \n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform(words)\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(columns=CountVec.get_feature_names())\n",
    "print(cv_dataframe)\n",
    "CountVec.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee4982-296d-4063-8f31-897f7e39dae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
